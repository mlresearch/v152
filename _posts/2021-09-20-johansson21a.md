---
title: Calibrating multi-class models
abstract: Predictive models communicating algorithmic confidence are very informative,
  but only  if well-calibrated and sharp, i.e., providing accurate probability estimates
  adjusted for each instance.  While almost all machine learning algorithms are able
  to produce probability estimates, these are  often poorly calibrated, thus requiring
  external calibration. For multiclass problems, external  calibration has typically
  been done using one-vs-all or all-vs-all schemes, thus adding to the  computational
  complexity, but also making it impossible to analyze and inspect the predictive  models.
  In this paper, we suggest a novel approach for calibrating inherently multi-class
  models.  Instead of providing a probability distribution over all labels, the estimation
  is of the probability that  the class label predicted by the underlying model is
  correct. In an extensive empirical study, it is  shown that the suggested approach,
  when applied to both Platt scaling and Venn-Abers, is able to  improve the probability
  estimates from decision trees, random forests and extreme gradient  boosting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: johansson21a
month: 0
tex_title: Calibrating multi-class models
firstpage: 111
lastpage: 130
page: 111-130
order: 111
cycles: false
bibtex_author: Johansson, Ulf and L\"{o}fstr\"om, Tuwe and Bostr\"om, Henrik
author:
- given: Ulf
  family: Johansson
- given: Tuwe
  family: Löfström
- given: Henrik
  family: Boström
date: 2021-09-20
address:
container-title: Proceedings of the Tenth Symposium on Conformal and Probabilistic
  Prediction and Applications
volume: '152'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 9
  - 20
pdf: https://proceedings.mlr.press/v152/johansson21a/johansson21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
