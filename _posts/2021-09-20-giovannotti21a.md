---
title: Transformer-based conformal predictors for paraphrase detection
abstract: Transformer architectures have established themselves as the state-of-the-art
  in many  areas of natural language processing (NLP), including paraphrase detection
  (PD). However, they do  not include a confidence estimation for each prediction
  and, in many cases, the applied models are  poorly calibrated. These features are
  essential for numerous real-world applications. For example, in  those cases when
  PD is used for sensitive tasks, like plagiarism detection, hate speech recognition
  or  in medical NLP, mistakes might be very costly. In this work we build several
  variants of transformer- based conformal predictors and study their behaviour on
  a standard PD dataset. We show that our  models are able to produce \emph{valid}
  predictions while retaining the accuracy of the original  transformer-based models.
  The proposed technique can be extended to many more NLP problems  that are currently
  being investigated.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: giovannotti21a
month: 0
tex_title: Transformer-based conformal predictors for paraphrase detection
firstpage: 243
lastpage: 265
page: 243-265
order: 243
cycles: false
bibtex_author: Giovannotti, Patrizio and Gammerman, Alex
author:
- given: Patrizio
  family: Giovannotti
- given: Alex
  family: Gammerman
date: 2021-09-20
address:
container-title: Proceedings of the Tenth Symposium on Conformal and Probabilistic
  Prediction and Applications
volume: '152'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 9
  - 20
pdf: https://proceedings.mlr.press/v152/giovannotti21a/giovannotti21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
